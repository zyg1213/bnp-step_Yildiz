{\rtf1\ansi\ansicpg1252\cocoartf2822
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;}
{\*\expandedcolortbl;;}
\margl1440\margr1440\vieww11520\viewh8400\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0\fs24 \cf0 Understanding bnp-step:\
\
Where is the step data?\
Line 1106 if bnpstep.py\
ax1.stairs(step_data, np.insert(T,0,T[0]), baseline=None, color=learncolor, linewidth=1.0, zorder=10)\
\
Line 1099\
step_data  = bnpa.get_step_plot_data(b_clean , h_clean , t_clean , f_clean, t_n , self.B_max , t_n.size , map_index)\
\
\
bnpanalysis.py Line 375\
Def get_step_plot_data(b_m_vec, h_m_vec, t_m_vec, f_vec, data_times, weak_limit, num_data, map_index)\
\
\
Where we get cleaned versions of these parameters by removing the burn-in:\
Line 867\
# Remove burn-in\
# TODO: allow an option for the user to display the log posterior and manually select burn-in point.\
# By default, remove the first 25% of samples as burn-in.\
burn_in_samples = int(0.25 * len(self.post))\
b_clean, h_clean, t_clean, f_clean, eta_clean, post_clean = bnpa.remove_burn_in(self.B_M, self.H_M, self.T_M, self.F_S, self.ETA, self.post, burn_in_samples)\
\
\
How do we get B_M and so on? What do they mean?\
We could use a simulated annealing protocol, but it is set for false by default. Instead use a Gibbs sampler which is why temperature is 1 for acceptance.\
https://gregorygundersen.com/blog/2020/02/23/gibbs-sampling/\
Line 367-407 of bnpstep.py\
\
I still want to know what is:\
self.ETA = []\
self.F_S = []\
self.B_M = []\
self.H_M = []\
self.T_M = []\
self.post = []\
\
\
Okay, time to read the paper. Mostly the same page as Figure 2\
\
Simplest case is a staircase of decay:\
F_bg is background estimation\
And we assume steps are w_n = s_n + F_bg + epsilon_n\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 s_n is actual signal, F_bg is offset, epsilon_n is noise\
Note that different noise models can be accommodated by sampling w_n from desired distribution\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
s_n = h_1 H(t_n - tau_1) + h_2 H(t_n - tau_2) + \'85\
\
Reformulate with Equation 2:\
s_n = sum_m^inf [(b_m h_m H(t_n - tau_m)]\
s_n is the raw signal seen\
\
b_m : binary to determine whether the m\'92th step contributes (hence why 0\'92s and 1\'92s)\
h_m : base step height\
T_m (i.e. tau_m) : time of the m\'92th step being turned on\
\
\'93Infinite numbers of steps are computationally cumbersome and ultimately\
unrealistic, so in practice we always truncate this infinite set of potential steps,\
with M exceeding a reasonable number of expected steps in the system.\'94\
\
\
\
Okay so now breaking down everything in my custom function sampler.export_step_data()\
\
Samples gives us histograms of the samples after burn in. Might be helpful to determine how important that step is.\
step_data is generated by doing the math illuminated above using bnpa.get_step_plot_data\
The return is the fitted MAP trace\
\
So if we want to rewrite a "FIONA" .mat file for reimport, we just need to put in place of trace(:,3) = step_data and then we are done\
So there might be a way to move this over into MATLAB\'85maybe. chatGPT would help. But I might also just try to run anaconda in the background at first.}